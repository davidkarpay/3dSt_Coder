Project Overview
Goal – Build a locally‑hosted, extensible “coding‑agent” that wraps an open‑source LLM with a mixture‑of‑experts (MoE) backbone and a set of developer‑oriented tools. The agent will be driven by a simple, modern GUI (desktop‑app) and expose a clean HTTP/JSON API that can be used from any IDE or script.

Target user – Full‑stack developers who want an offline, privacy‑first AI partner that can:

Generate, refactor, and explain code in many languages.
Run, test and debug snippets in a sandbox.
Interact with the local file system, Git, package managers, linters, CI pipelines, etc.
Keep a “conversation memory” across a project.
The solution is built by‑design with:

Test‑Driven Development (TDD) – every feature is first described by failing tests.
Clean‑Code & SOLID – each module has a single responsibility, clear interfaces and expressive naming.
Code‑Complete – every public API is fully typed, documented and covered by unit/integration tests.
The Pragmatic Programmer mindset – “the right tool for the job”, automation, continuous refactoring, and “you aren’t gonna need it (YAGNI)”.
1. Architectural Validation (Phase 0)
Question	How we answer it	Acceptance Criterion
Can a MoE LLM run locally on consumer hardware?	Use an open‑source MoE model such as DeepSeek‑MoE‑Coder (see DeepSeek MoE explanation) which activates only a fraction of its billions of parameters per request
[1]
.	A single‑GPU (e.g., RTX 3080) can serve 1–2 requests/sec with <2 GB VRAM per request (measured with vLLM).
Is the inference latency acceptable for an interactive coding assistant?	Benchmark with vLLM (GPU‑accelerated, token‑streaming).	95 %‑ile latency ≤ 800 ms for a 150‑token reply.
Do the tool‑integration patterns (LangChain‑style “tools”) work with a local agent?	Prototype a minimal ReAct loop that calls a FileReadTool and a GitStatusTool.	End‑to‑end test passes: “show me the repo status” returns a correct git status output.
Can a lightweight desktop UI communicate with the backend?	Build a tiny Tauri front‑end that talks to a local FastAPI server over http://127.0.0.1:8000/api.	GUI starts in ≤ 2 s, sends a request and displays the streamed response without blocking.
Is the overall architecture testable with CI?	All components are pure‑Python (except UI) and executable in a GitHub Actions runner.	pytest runs > 90 % coverage on PRs, builds and pushes a Docker image.
Result: All validation experiments succeed → move to Phase 1 (core engine).

2. Technology Stack
Layer	Primary Technology	Rationale
LLM Inference	vLLM (CUDA‑accelerated, supports MoE) + HuggingFace Transformers	Fast, token‑streaming, easy to swap models.
Model	DeepSeek‑MoE‑Coder (or similar open‑source MoE code model)	MoE gives high quality with lower compute cost
[1]
.
Agent orchestration	LangChain‑Core (tool‑calling, ReAct, memory)	Proven pattern, fully testable, language‑agnostic.
Backend API	FastAPI (Python 3.12)	Async, OpenAPI schema, automatic docs, easy to test.
Tool implementations	Pure‑Python packages:
• subprocess sandbox (Docker‑or‑podman)
• GitPython
• pyflakes / black
• pytest
• pydantic for config	All have solid unit‑test support.
Persistence / Memory	SQLite (via SQLModel) for project‑level chat history	Lightweight, file‑based, no external DB.
Desktop GUI	Tauri (Rust core + React/TypeScript front‑end)	Native binary, very small bundle (< 10 MB), low memory, web‑tech UI.
CI/CD	GitHub Actions + Poetry for dependency management	Reproducible builds, test matrix on Ubuntu/Windows.
Packaging	Docker (docker build -t coding‑agent:latest) + optional Conda env for users without Docker.	Guarantees a reproducible local runtime.
3. Phased Implementation Plan
Below is a road‑map that gradually expands scope while keeping a tight test loop.

Phase	Scope	Primary Deliverable	Key Tests (TDD)
0 – Validation	Benchmarks, proof‑of‑concept scripts	benchmark.ipynb, demo_agent.py	Performance test (test_latency), tool‑call test (test_file_read).
1 – Core Engine	• LLM server (vLLM wrapper)
• Minimal ReAct agent (prompt + tool‑router)	llm_server/ (FastAPI + vLLM)	test_llm_endpoint, test_agent_basic_loop.
2 – Tool Suite	Implement a catalogue of secure tools:
File I/O, Git, Shell (container‑isolated), Linter, Test Runner, Docs generator.	tools/ package, auto‑discovery via entry‑points	For each tool: unit (e.g., test_git_status) + contract (JSON schema).
3 – Memory & Project Context	Persistent chat history, token‑budget‑aware truncation, project‑root detection.	memory/ (SQLModel + LangChain memory)	test_memory_persistence, test_context_window.
4 – API Layer	Full OpenAPI spec, streaming responses, auth token (optional).	api/ module, autogenerated docs (/docs)	test_openapi_schema, test_streaming_response.
5 – Desktop GUI	Tauri frontend with:
• Chat pane (streaming)
• File explorer sidebar
• Settings dialog (model path, GPU, tool toggles)	ui/ (React+TS) + ui/src-tauri/ (Rust bridge)	End‑to‑end Cypress tests: test_send_message, test_file_tree.
6 – CI / Packaging	GitHub Actions workflow, Dockerfile, Poetry lock, release scripts.	CI pipeline badge, Docker image on GHCR, optional conda env export.	test_ci_success, test_docker_entrypoint.
7 – Production‑Ready Extras	• Plugin system (Python entry‑points)
• Remote‑sync (optional gRPC bridge)
• Telemetry (opt‑in)	Plugin SDK, docs, versioned releases.	test_plugin_loading, test_telemetry_optout.
Each phase ends with a “green‑light” demo where all tests pass, code coverage ≥ 90 % and a short user‑story is demonstrated (e.g., “refactor a function and run its tests”).

4. Detailed Component Design
4.1 LLM Inference Service
llm_server/
│─ main.py          # FastAPI entry point
│─ inference.py     # vLLM wrapper class
│─ config.py        # Pydantic settings (model_path, gpu_id, max_tokens)
│─ schemas.py       # Request/Response Pydantic models
│─ tests/
│   └─ test_inference.py
Key class (TDD first):

# inference.py
class VLLMEngine:
    """Thin wrapper around vLLM that hides streaming details."""

    def __init__(self, cfg: LLMConfig):
        self.cfg = cfg
        self._engine = None   # lazy init

    async def generate(self, prompt: str, stop: List[str] | None = None) -> AsyncIterator[str]:
        """Yield tokens as they become available."""
        if self._engine is None:
            self._engine = await vllm.AsyncEngine.from_pretrained(
                cfg.model_path,
                gpu_ids=[cfg.gpu_id],
                max_model_len=cfg.max_context,
                # MoE‑specific flags...
            )
        async for token in self._engine.generate(prompt, stop=stop):
            yield token
Test (written before implementation):

# tests/test_inference.py
@pytest.mark.asyncio
async def test_generate_streams_tokens(fake_cfg):
    engine = VLLMEngine(fake_cfg)
    tokens = []
    async for t in engine.generate("def hello(): pass"):
        tokens.append(t)
    assert "".join(tokens).startswith("def")
4.2 Agent & Tool Orchestration
agent/
│─ core.py          # ReAct loop implementation
│─ tools/
│   ├─ __init__.py
│   ├─ file.py
│   ├─ git.py
│   ├─ shell.py
│   └─ test_runner.py
│─ memory.py        # LangChain memory wrapper
│─ schemas.py
│─ tests/
│   └─ test_core.py
ReAct loop (TDD):

# core.py
class CodingAgent:
    def __init__(self, llm: VLLMEngine, tools: Dict[str, BaseTool], memory: BaseMemory):
        self.llm = llm
        self.tools = tools
        self.memory = memory

    async def chat(self, user_msg: str) -> AsyncIterator[str]:
        # 1. Append to memory
        await self.memory.add_user_message(user_msg)

        # 2. Generate LLM output (may include tool calls)
        async for chunk in self.llm.generate(self._build_prompt()):
            # Detect tool call pattern e.g. `{{tool:git_status}}`
            if tool_name := self._extract_tool(chunk):
                result = await self.tools[tool_name].run()
                await self.memory.add_tool_result(tool_name, result)
                # Re‑inject result and continue generation
                continue
            yield chunk
Tool contract (base class):

class BaseTool(Protocol):
    name: str
    description: str

    async def run(self, *args: Any, **kwargs: Any) -> str:
        ...
Example tool (file read) – test first:

# tests/tools/test_file.py
def test_file_read(tmp_path):
    p = tmp_path / "sample.py"
    p.write_text("print('hello')")
    tool = FileReadTool()
    result = asyncio.run(tool.run(path=str(p)))
    assert "print('hello')" in result
4.3 API Layer
api/
│─ router.py        # FastAPI router exposing /chat, /tools, /history
│─ dependencies.py  # DI for llm, agent, memory
│─ main.py          # uvicorn entry point
│─ tests/
│   └─ test_api.py
Streaming endpoint (OpenAPI spec generated automatically):

@router.post("/chat", response_model=ChatStreamResponse, tags=["Chat"])
async def chat_endpoint(req: ChatRequest, 
                        agent: CodingAgent = Depends(get_agent)):
    async def event_generator():
        async for chunk in agent.chat(req.message):
            yield f"data: {json.dumps({'delta': chunk})}\n\n"
    return EventSourceResponse(event_generator())
Test (client‑side simulation):

def test_chat_stream(client):
    resp = client.post("/chat", json={"message": "list repo files"}, stream=True)
    data = b"".join(resp.iter_content())
    assert b"git status" in data or b".py" in data
4.4 Desktop GUI (Tauri)
Folder layout:

ui/
│─ src/
│   ├─ App.tsx
│   ├─ components/
│   │   ├─ Chat.tsx        # streaming UI
│   │   ├─ FileTree.tsx
│   │   └─ Settings.tsx
│   └─ api/
│       └─ client.ts       # wrappers around http://127.0.0.1:8000
│─ src-tauri/
│   └─ main.rs            # launches FastAPI subprocess, handles exit
│─ public/
│   └─ index.html
│─ tests/
│   └─ e2e/
│       └─ chat_spec.cy.ts
Key UI flow (pseudo‑code):

// Chat.tsx
const [messages, setMessages] = useState<Message[]>([]);

const send = async (text: string) => {
  const resp = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({message: text}),
    headers: {'Content-Type': 'application/json'}
  });
  const reader = resp.body!.getReader();
  // stream chunks → append to last message
  while (true) {
    const {done, value} = await reader.read();
    if (done) break;
    const delta = new TextDecoder().decode(value);
    setMessages(prev => appendDelta(prev, delta));
  }
};
TDD for UI (Cypress):

it('should display assistant response while streaming', () => {
  cy.visit('/');
  cy.get('[data-cy=message-input]').type('show me the README{enter}');
  cy.get('[data-cy=assistant-msg]').should('contain.text', 'README.md');
});
4.5 CI / Packaging
.github/workflows/ci.yml

name: CI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install Poetry
        run: pip install poetry
      - name: Install deps
        run: poetry install --with dev
      - name: Run tests
        run: poetry run pytest -n auto --cov=.
      - name: Lint
        run: poetry run ruff check .
  docker:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build image
        run: |
          docker build -t ghcr.io/${{ github.repository }}/coding-agent:latest .
      - name: Push image
        if: github.ref == 'refs/heads/main'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - run: |
          docker push ghcr.io/${{ github.repository }}/coding-agent:latest
Dockerfile (multi‑stage):

FROM python:3.12-slim AS builder
WORKDIR /app
COPY poetry.lock pyproject.toml ./
RUN pip install poetry && poetry install --no-root --only main
COPY . .
RUN poetry build

FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04
WORKDIR /app
COPY --from=builder /app/dist/*.whl .
RUN pip install *.whl
EXPOSE 8000
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
5. Development Process (TDD + Clean‑Code Workflow)
Write a failing test – e.g., test_git_status.
Run test → red.
Implement minimal production code to make it pass.
Refactor (extract methods, rename, add type hints) while keeping the test green.
Add documentation & doc‑strings ("""Explain why this tool does X.""").
Run full test suite + coverage; enforce >=90 % with pytest-cov.
Commit with a descriptive message: feat(tool): add git status tool.
Open PR; CI runs automatically; reviewer checks that tests cover new branches and that no lint errors exist.
All public functions are typed (def foo(a: str) -> int:) and have a docstring.

Every module follows the “single responsibility” principle – the tools package never knows about the LLM, and the agent never accesses the filesystem directly.

6. Security & Sandbox Considerations
Risk	Mitigation (Pragmatic)
Arbitrary code execution by the LLM	All shell/python tool calls are executed inside a Docker container with read‑only mounts, limited CPU/memory, and no network.
File system leakage	Tools expose only paths under the discovered project root. Path traversal is blocked (os.path.commonpath).
Model poisoning / prompt injection	LLM prompts are sanitized – any {{tool:…}} pattern is parsed via a strict regex; unknown patterns are ignored.
Data persistence	Chat history stored locally in an encrypted SQLite DB (optional pass‑phrase). No telemetry is sent without user consent.
7. Sample End‑to‑End User Story (Demo)
User opens the desktop app → UI boots the FastAPI backend (shown in Tauri main.rs).
In the chat pane they type: “Create a new api/users FastAPI router with CRUD endpoints, then run the unit tests.”
The agent:
Generates scaffold code using the MoE model.
Calls the FileWriteTool to create api/users.py.
Calls TestRunnerTool to execute pytest.
Returns the test results as a streamed message.
The user clicks the “Open File” button in the file‑tree sidebar – the newly created file opens in the system editor.
All steps are verified by integration tests that mock the LLM (using a deterministic stub) and assert that the correct tools were invoked in the right order.

8. Future Extensions (Post‑MVP)
Feature	Why it fits the Pragmatic Programmer ethos
Plugin SDK (Python entry points)	Allows developers to add domain‑specific tools without touching core code.
Remote‑agent mode (gRPC)	Lets the same backend be used by VS Code extensions, CI pipelines, or headless servers.
Model hot‑swap (multiple MoE or dense models)	Future‑proof: as better open‑source models appear, swap with zero code change.
Explain‑by‑example – tool that shows “why this line was suggested”	Improves developer trust, embraces “explain the code you write”.
Telemetry opt‑in dashboard	Gives maintainers data for performance tuning while respecting privacy.
9. Deliverables Checklist
Item	Status (by phase)
✅ Architecture validation report	Phase 0
✅ vLLM inference service (unit‑tested)	Phase 1
✅ ReAct agent core (TDD)	Phase 1
✅ Full toolbox (file, git, shell, test, lint)	Phase 2
✅ Persistent memory + context truncation	Phase 3
✅ FastAPI with OpenAPI spec & streaming	Phase 4
✅ Tauri UI (chat, file tree, settings)	Phase 5
✅ GitHub Actions CI, Docker image, Poetry lock	Phase 6
✅ Plugin SDK & docs (optional)	Phase 7
📦 Release zip for Windows/macOS/Linux (self‑contained)	End of Phase 6
10. Getting Started (for the development team)
# 1. Clone repo and set up environment
git clone https://github.com/yourorg/coding-agent.git
cd coding-agent
poetry install   # installs fastapi, vllm, langchain-core, etc.

# 2. Run the LLM server (replace with actual DeepSeek‑MoE model path)
poetry run python -m llm_server.main --model /models/deepseek-moe-coder

# 3. In another terminal, start the API
poetry run uvicorn api.main:app --reload

# 4. Launch the UI (requires Node & Tauri CLI)
cd ui
npm install
npm run tauri dev   # opens the desktop app
Run the full test suite:

poetry run pytest -n auto --cov=.
All new features must be accompanied by a failing test first; the CI pipeline will enforce this.

In a nutshell – this plan delivers a fully‑scoped, test‑first, clean‑coded, locally‑hosted coding agent that wraps a powerful MoE LLM, a curated toolbox, and a modern desktop UI. The phased roadmap, explicit testing strategy, and pragmatic security choices make it both developer‑friendly and production‑ready.

Downloading model