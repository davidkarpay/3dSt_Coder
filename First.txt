Project Overview
Goal â€“ Build a locallyâ€‘hosted, extensible â€œcodingâ€‘agentâ€ that wraps an openâ€‘source LLM with a mixtureâ€‘ofâ€‘experts (MoE) backbone and a set of developerâ€‘oriented tools. The agent will be driven by a simple, modern GUI (desktopâ€‘app) and expose a clean HTTP/JSON API that can be used from any IDE or script.

Target user â€“ Fullâ€‘stack developers who want an offline, privacyâ€‘first AI partner that can:

Generate, refactor, and explain code in many languages.
Run, test and debug snippets in a sandbox.
Interact with the local file system, Git, package managers, linters, CI pipelines, etc.
Keep a â€œconversation memoryâ€ across a project.
The solution is built byâ€‘design with:

Testâ€‘Driven Development (TDD) â€“ every feature is first described by failing tests.
Cleanâ€‘Code & SOLID â€“ each module has a single responsibility, clear interfaces and expressive naming.
Codeâ€‘Complete â€“ every public API is fully typed, documented and covered by unit/integration tests.
The Pragmatic Programmer mindset â€“ â€œthe right tool for the jobâ€, automation, continuous refactoring, and â€œyou arenâ€™t gonna need it (YAGNI)â€.
1. Architectural Validation (Phaseâ€¯0)
Question	How we answer it	Acceptance Criterion
Can a MoE LLM run locally on consumer hardware?	Use an openâ€‘source MoE model such as DeepSeekâ€‘MoEâ€‘Coder (see DeepSeek MoE explanation) which activates only a fraction of its billions of parameters per request
[1]
.	A singleâ€‘GPU (e.g., RTXâ€¯3080) can serve 1â€“2 requests/sec with <2â€¯GB VRAM per request (measured with vLLM).
Is the inference latency acceptable for an interactive coding assistant?	Benchmark with vLLM (GPUâ€‘accelerated, tokenâ€‘streaming).	95â€¯%â€‘ile latency â‰¤â€¯800â€¯ms for a 150â€‘token reply.
Do the toolâ€‘integration patterns (LangChainâ€‘style â€œtoolsâ€) work with a local agent?	Prototype a minimal ReAct loop that calls a FileReadTool and a GitStatusTool.	Endâ€‘toâ€‘end test passes: â€œshow me the repo statusâ€ returns a correct git status output.
Can a lightweight desktop UI communicate with the backend?	Build a tiny Tauri frontâ€‘end that talks to a local FastAPI server over http://127.0.0.1:8000/api.	GUI starts in â‰¤â€¯2â€¯s, sends a request and displays the streamed response without blocking.
Is the overall architecture testable with CI?	All components are pureâ€‘Python (except UI) and executable in a GitHub Actions runner.	pytest runs >â€¯90â€¯% coverage on PRs, builds and pushes a Docker image.
Result: All validation experiments succeed â†’ move to Phaseâ€¯1 (core engine).

2. Technology Stack
Layer	Primary Technology	Rationale
LLM Inference	vLLM (CUDAâ€‘accelerated, supports MoE) + HuggingFace Transformers	Fast, tokenâ€‘streaming, easy to swap models.
Model	DeepSeekâ€‘MoEâ€‘Coder (or similar openâ€‘source MoE code model)	MoE gives high quality with lower compute cost
[1]
.
Agent orchestration	LangChainâ€‘Core (toolâ€‘calling, ReAct, memory)	Proven pattern, fully testable, languageâ€‘agnostic.
Backend API	FastAPI (Pythonâ€¯3.12)	Async, OpenAPI schema, automatic docs, easy to test.
Tool implementations	Pureâ€‘Python packages:
â€¢ subprocess sandbox (Dockerâ€‘orâ€‘podman)
â€¢ GitPython
â€¢ pyflakes / black
â€¢ pytest
â€¢ pydantic for config	All have solid unitâ€‘test support.
Persistence / Memory	SQLite (via SQLModel) for projectâ€‘level chat history	Lightweight, fileâ€‘based, no external DB.
Desktop GUI	Tauri (Rust core + React/TypeScript frontâ€‘end)	Native binary, very small bundle (<â€¯10â€¯MB), low memory, webâ€‘tech UI.
CI/CD	GitHub Actions + Poetry for dependency management	Reproducible builds, test matrix on Ubuntu/Windows.
Packaging	Docker (docker build -t codingâ€‘agent:latest) + optional Conda env for users without Docker.	Guarantees a reproducible local runtime.
3. Phased Implementation Plan
Below is a roadâ€‘map that gradually expands scope while keeping a tight test loop.

Phase	Scope	Primary Deliverable	Key Tests (TDD)
0 â€“ Validation	Benchmarks, proofâ€‘ofâ€‘concept scripts	benchmark.ipynb, demo_agent.py	Performance test (test_latency), toolâ€‘call test (test_file_read).
1 â€“ Core Engine	â€¢ LLM server (vLLM wrapper)
â€¢ Minimal ReAct agent (prompt + toolâ€‘router)	llm_server/ (FastAPI + vLLM)	test_llm_endpoint, test_agent_basic_loop.
2 â€“ Tool Suite	Implement a catalogue of secure tools:
File I/O, Git, Shell (containerâ€‘isolated), Linter, Test Runner, Docs generator.	tools/ package, autoâ€‘discovery via entryâ€‘points	For each tool: unit (e.g., test_git_status) + contract (JSON schema).
3 â€“ Memory & Project Context	Persistent chat history, tokenâ€‘budgetâ€‘aware truncation, projectâ€‘root detection.	memory/ (SQLModel + LangChain memory)	test_memory_persistence, test_context_window.
4 â€“ API Layer	Full OpenAPI spec, streaming responses, auth token (optional).	api/ module, autogenerated docs (/docs)	test_openapi_schema, test_streaming_response.
5 â€“ Desktop GUI	Tauri frontend with:
â€¢ Chat pane (streaming)
â€¢ File explorer sidebar
â€¢ Settings dialog (model path, GPU, tool toggles)	ui/ (React+TS) + ui/src-tauri/ (Rust bridge)	Endâ€‘toâ€‘end Cypress tests: test_send_message, test_file_tree.
6 â€“ CI / Packaging	GitHub Actions workflow, Dockerfile, Poetry lock, release scripts.	CI pipeline badge, Docker image on GHCR, optional conda env export.	test_ci_success, test_docker_entrypoint.
7 â€“ Productionâ€‘Ready Extras	â€¢ Plugin system (Python entryâ€‘points)
â€¢ Remoteâ€‘sync (optional gRPC bridge)
â€¢ Telemetry (optâ€‘in)	Plugin SDK, docs, versioned releases.	test_plugin_loading, test_telemetry_optout.
Each phase ends with a â€œgreenâ€‘lightâ€ demo where all tests pass, code coverage â‰¥â€¯90â€¯% and a short userâ€‘story is demonstrated (e.g., â€œrefactor a function and run its testsâ€).

4. Detailed Component Design
4.1 LLM Inference Service
llm_server/
â”‚â”€ main.py          # FastAPI entry point
â”‚â”€ inference.py     # vLLM wrapper class
â”‚â”€ config.py        # Pydantic settings (model_path, gpu_id, max_tokens)
â”‚â”€ schemas.py       # Request/Response Pydantic models
â”‚â”€ tests/
â”‚   â””â”€ test_inference.py
Key class (TDD first):

# inference.py
class VLLMEngine:
    """Thin wrapper around vLLM that hides streaming details."""

    def __init__(self, cfg: LLMConfig):
        self.cfg = cfg
        self._engine = None   # lazy init

    async def generate(self, prompt: str, stop: List[str] | None = None) -> AsyncIterator[str]:
        """Yield tokens as they become available."""
        if self._engine is None:
            self._engine = await vllm.AsyncEngine.from_pretrained(
                cfg.model_path,
                gpu_ids=[cfg.gpu_id],
                max_model_len=cfg.max_context,
                # MoEâ€‘specific flags...
            )
        async for token in self._engine.generate(prompt, stop=stop):
            yield token
Test (written before implementation):

# tests/test_inference.py
@pytest.mark.asyncio
async def test_generate_streams_tokens(fake_cfg):
    engine = VLLMEngine(fake_cfg)
    tokens = []
    async for t in engine.generate("def hello(): pass"):
        tokens.append(t)
    assert "".join(tokens).startswith("def")
4.2 Agent & Tool Orchestration
agent/
â”‚â”€ core.py          # ReAct loop implementation
â”‚â”€ tools/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ file.py
â”‚   â”œâ”€ git.py
â”‚   â”œâ”€ shell.py
â”‚   â””â”€ test_runner.py
â”‚â”€ memory.py        # LangChain memory wrapper
â”‚â”€ schemas.py
â”‚â”€ tests/
â”‚   â””â”€ test_core.py
ReAct loop (TDD):

# core.py
class CodingAgent:
    def __init__(self, llm: VLLMEngine, tools: Dict[str, BaseTool], memory: BaseMemory):
        self.llm = llm
        self.tools = tools
        self.memory = memory

    async def chat(self, user_msg: str) -> AsyncIterator[str]:
        # 1. Append to memory
        await self.memory.add_user_message(user_msg)

        # 2. Generate LLM output (may include tool calls)
        async for chunk in self.llm.generate(self._build_prompt()):
            # Detect tool call pattern e.g. `{{tool:git_status}}`
            if tool_name := self._extract_tool(chunk):
                result = await self.tools[tool_name].run()
                await self.memory.add_tool_result(tool_name, result)
                # Reâ€‘inject result and continue generation
                continue
            yield chunk
Tool contract (base class):

class BaseTool(Protocol):
    name: str
    description: str

    async def run(self, *args: Any, **kwargs: Any) -> str:
        ...
Example tool (file read) â€“ test first:

# tests/tools/test_file.py
def test_file_read(tmp_path):
    p = tmp_path / "sample.py"
    p.write_text("print('hello')")
    tool = FileReadTool()
    result = asyncio.run(tool.run(path=str(p)))
    assert "print('hello')" in result
4.3 API Layer
api/
â”‚â”€ router.py        # FastAPI router exposing /chat, /tools, /history
â”‚â”€ dependencies.py  # DI for llm, agent, memory
â”‚â”€ main.py          # uvicorn entry point
â”‚â”€ tests/
â”‚   â””â”€ test_api.py
Streaming endpoint (OpenAPI spec generated automatically):

@router.post("/chat", response_model=ChatStreamResponse, tags=["Chat"])
async def chat_endpoint(req: ChatRequest, 
                        agent: CodingAgent = Depends(get_agent)):
    async def event_generator():
        async for chunk in agent.chat(req.message):
            yield f"data: {json.dumps({'delta': chunk})}\n\n"
    return EventSourceResponse(event_generator())
Test (clientâ€‘side simulation):

def test_chat_stream(client):
    resp = client.post("/chat", json={"message": "list repo files"}, stream=True)
    data = b"".join(resp.iter_content())
    assert b"git status" in data or b".py" in data
4.4 Desktop GUI (Tauri)
Folder layout:

ui/
â”‚â”€ src/
â”‚   â”œâ”€ App.tsx
â”‚   â”œâ”€ components/
â”‚   â”‚   â”œâ”€ Chat.tsx        # streaming UI
â”‚   â”‚   â”œâ”€ FileTree.tsx
â”‚   â”‚   â””â”€ Settings.tsx
â”‚   â””â”€ api/
â”‚       â””â”€ client.ts       # wrappers around http://127.0.0.1:8000
â”‚â”€ src-tauri/
â”‚   â””â”€ main.rs            # launches FastAPI subprocess, handles exit
â”‚â”€ public/
â”‚   â””â”€ index.html
â”‚â”€ tests/
â”‚   â””â”€ e2e/
â”‚       â””â”€ chat_spec.cy.ts
Key UI flow (pseudoâ€‘code):

// Chat.tsx
const [messages, setMessages] = useState<Message[]>([]);

const send = async (text: string) => {
  const resp = await fetch('/api/chat', {
    method: 'POST',
    body: JSON.stringify({message: text}),
    headers: {'Content-Type': 'application/json'}
  });
  const reader = resp.body!.getReader();
  // stream chunks â†’ append to last message
  while (true) {
    const {done, value} = await reader.read();
    if (done) break;
    const delta = new TextDecoder().decode(value);
    setMessages(prev => appendDelta(prev, delta));
  }
};
TDD for UI (Cypress):

it('should display assistant response while streaming', () => {
  cy.visit('/');
  cy.get('[data-cy=message-input]').type('show me the README{enter}');
  cy.get('[data-cy=assistant-msg]').should('contain.text', 'README.md');
});
4.5 CI / Packaging
.github/workflows/ci.yml

name: CI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install Poetry
        run: pip install poetry
      - name: Install deps
        run: poetry install --with dev
      - name: Run tests
        run: poetry run pytest -n auto --cov=.
      - name: Lint
        run: poetry run ruff check .
  docker:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build image
        run: |
          docker build -t ghcr.io/${{ github.repository }}/coding-agent:latest .
      - name: Push image
        if: github.ref == 'refs/heads/main'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - run: |
          docker push ghcr.io/${{ github.repository }}/coding-agent:latest
Dockerfile (multiâ€‘stage):

FROM python:3.12-slim AS builder
WORKDIR /app
COPY poetry.lock pyproject.toml ./
RUN pip install poetry && poetry install --no-root --only main
COPY . .
RUN poetry build

FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04
WORKDIR /app
COPY --from=builder /app/dist/*.whl .
RUN pip install *.whl
EXPOSE 8000
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
5. Development Process (TDD + Cleanâ€‘Code Workflow)
Write a failing test â€“ e.g., test_git_status.
Run test â†’ red.
Implement minimal production code to make it pass.
Refactor (extract methods, rename, add type hints) while keeping the test green.
Add documentation & docâ€‘strings ("""Explain why this tool does X.""").
Run full test suite + coverage; enforce >=90â€¯% with pytest-cov.
Commit with a descriptive message: feat(tool): add git status tool.
Open PR; CI runs automatically; reviewer checks that tests cover new branches and that no lint errors exist.
All public functions are typed (def foo(a: str) -> int:) and have a docstring.

Every module follows the â€œsingle responsibilityâ€ principle â€“ the tools package never knows about the LLM, and the agent never accesses the filesystem directly.

6. Security & Sandbox Considerations
Risk	Mitigation (Pragmatic)
Arbitrary code execution by the LLM	All shell/python tool calls are executed inside a Docker container with readâ€‘only mounts, limited CPU/memory, and no network.
File system leakage	Tools expose only paths under the discovered project root. Path traversal is blocked (os.path.commonpath).
Model poisoning / prompt injection	LLM prompts are sanitized â€“ any {{tool:â€¦}} pattern is parsed via a strict regex; unknown patterns are ignored.
Data persistence	Chat history stored locally in an encrypted SQLite DB (optional passâ€‘phrase). No telemetry is sent without user consent.
7. Sample Endâ€‘toâ€‘End User Story (Demo)
User opens the desktop app â†’ UI boots the FastAPI backend (shown in Tauri main.rs).
In the chat pane they type: â€œCreate a new api/users FastAPI router with CRUD endpoints, then run the unit tests.â€
The agent:
Generates scaffold code using the MoE model.
Calls the FileWriteTool to create api/users.py.
Calls TestRunnerTool to execute pytest.
Returns the test results as a streamed message.
The user clicks the â€œOpen Fileâ€ button in the fileâ€‘tree sidebar â€“ the newly created file opens in the system editor.
All steps are verified by integration tests that mock the LLM (using a deterministic stub) and assert that the correct tools were invoked in the right order.

8. Future Extensions (Postâ€‘MVP)
Feature	Why it fits the Pragmatic Programmer ethos
Plugin SDK (Python entry points)	Allows developers to add domainâ€‘specific tools without touching core code.
Remoteâ€‘agent mode (gRPC)	Lets the same backend be used by VSâ€¯Code extensions, CI pipelines, or headless servers.
Model hotâ€‘swap (multiple MoE or dense models)	Futureâ€‘proof: as better openâ€‘source models appear, swap with zero code change.
Explainâ€‘byâ€‘example â€“ tool that shows â€œwhy this line was suggestedâ€	Improves developer trust, embraces â€œexplain the code you writeâ€.
Telemetry optâ€‘in dashboard	Gives maintainers data for performance tuning while respecting privacy.
9. Deliverables Checklist
Item	Status (by phase)
âœ… Architecture validation report	Phaseâ€¯0
âœ… vLLM inference service (unitâ€‘tested)	Phaseâ€¯1
âœ… ReAct agent core (TDD)	Phaseâ€¯1
âœ… Full toolbox (file, git, shell, test, lint)	Phaseâ€¯2
âœ… Persistent memory + context truncation	Phaseâ€¯3
âœ… FastAPI with OpenAPI spec & streaming	Phaseâ€¯4
âœ… Tauri UI (chat, file tree, settings)	Phaseâ€¯5
âœ… GitHub Actions CI, Docker image, Poetry lock	Phaseâ€¯6
âœ… Plugin SDK & docs (optional)	Phaseâ€¯7
ğŸ“¦ Release zip for Windows/macOS/Linux (selfâ€‘contained)	End of Phaseâ€¯6
10. Getting Started (for the development team)
# 1. Clone repo and set up environment
git clone https://github.com/yourorg/coding-agent.git
cd coding-agent
poetry install   # installs fastapi, vllm, langchain-core, etc.

# 2. Run the LLM server (replace with actual DeepSeekâ€‘MoE model path)
poetry run python -m llm_server.main --model /models/deepseek-moe-coder

# 3. In another terminal, start the API
poetry run uvicorn api.main:app --reload

# 4. Launch the UI (requires Node & Tauri CLI)
cd ui
npm install
npm run tauri dev   # opens the desktop app
Run the full test suite:

poetry run pytest -n auto --cov=.
All new features must be accompanied by a failing test first; the CI pipeline will enforce this.

In a nutshell â€“ this plan delivers a fullyâ€‘scoped, testâ€‘first, cleanâ€‘coded, locallyâ€‘hosted coding agent that wraps a powerful MoE LLM, a curated toolbox, and a modern desktop UI. The phased roadmap, explicit testing strategy, and pragmatic security choices make it both developerâ€‘friendly and productionâ€‘ready.

Downloading model